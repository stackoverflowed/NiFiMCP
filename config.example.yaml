nifi:
  servers:
    - id: "nifi-local-example" # Unique internal ID, used by the client/API calls
      name: "Local NiFi Example" # Display name for UI dropdown
      url: "https://localhost:8443/nifi-api" # URL to NiFi API
      username: "" # Optional: Username for NiFi basic auth
      password: "" # Optional: Password for NiFi basic auth - DO NOT COMMIT REAL PASSWORDS
      tls_verify: false # Set to true for valid certs, false for self-signed (dev only)
    # Add more NiFi server configurations here as needed
    # - id: "nifi-dev-example"
    #   name: "Development NiFi Example"
    #   url: "https://dev-nifi.example.com/nifi-api"
    #   username: "dev_user"
    #   password: "dev_password_env_var_reference_or_secret" # Example: Placeholder, ideally use env vars or secrets management
    #   tls_verify: true

llm:
  google:
    api_key: "" # Your Google API Key (e.g., AIza...)
    models: ["gemini-2.5-flash-lite-preview-06-17", "gemini-2.5-flash-preview-04-17", "gemini-2.5-flash", "gemini-2.5-pro"] # Comma-separated list of allowed models
  openai:
    api_key: "" # Your OpenAI API Key (e.g., sk-...)
    models: ["o4-mini","gpt-4.5-preview", "gpt-4.1", "gpt-4o", "o1", "o3", "gpt-4.1-mini", "gpt-4.1-nano", "gpt-4o-mini", "o1-mini", "o3-mini"] # Comma-separated list of allowed models
  perplexity:
    api_key: "" # Your Perplexity API Key (e.g., pplx-...)
    models: ["sonar-pro", "sonar", "sonar-reasoning-pro", "sonar-reasoning"] # Comma-separated list of allowed models
  anthropic:
    api_key: "" # Your Anthropic API Key (e.g., sk-ant-...)
    models: ["claude-opus-4-20250514", "claude-sonnet-4-20250109", "claude-3-5-haiku-20241022"] # Comma-separated list of allowed models
  expert_help_model:
    provider: "perplexity" # Choose from: openai, google, perplexity, anthropic
    model: "sonar-pro" # Must be one of the models listed above for the chosen provider

mcp_features:
  auto_stop_enabled: true
  auto_delete_enabled: true
  auto_purge_enabled: true

# Logging configuration
logging:
  # Enable/disable thread-safe logging queue for LLM debug logs
  # Setting to false may resolve pickle serialization errors with Google Protocol Buffers
  # but may reduce thread safety in high-concurrency scenarios
  llm_enqueue_enabled: true  # Set to false if experiencing MapComposite pickle errors

# general:
#   setting_1: "value" # Example for other potential app settings 